# DIY Star Wars GPT

A personal project to train a 10M-parameter Transformer model from scratch on the Star Wars saga. This project utilizes the **novelizations** of the movies rather than raw screenplays to provide a richer, more descriptive linguistic dataset. Cumulative text data used to train WangGPT is about 805,904 tokens, with a vocabulary size of about 37,159 words.

## Project Goal
Compare the loss curves of three model variants (Random Baseline, No Positional Encoding, and Full Transformer) while strictly adhering to a 10,000,000 parameter limit via a dynamic quadratic solver.

## Dataset & Processing
The final training set, `data/combined_star_wars.txt`, is automatically generated by:
1.  **Consolidating** source texts from `data/source_texts/` in numerical order (based on integer prefix).
2.  **ASCII Filtering**: Stripping all non-ASCII characters to ensure a clean, standardized vocabulary.
3.  **Separation**: Each source text is separated by two newlines (`\n\n`) to preserve distinct thematic boundaries.

## Setup
Ensure you have `uv` installed, then run:
```bash
uv sync
```

## Training
To execute the full comparative study (8 grid search runs + baseline):
```bash
uv run train_model.py
```
This generates:
- `models/random_init.pt`
- `models/no_pe_best.pt`
- `models/with_pe_best.pt`
- `training_comparison.png`

## Chat
Interact with any of your trained "holocrons" via the CLI:
```bash
uv run chat.py
```

## Key Files
- `wang_gpt.py`: Core architecture (7-layer Multi-Head Attention).
- `train_model.py`: Training orchestrator with grid search.
- `utils.py`: Tokenization and vocabulary handling.
- `unify_vocabs.py`: Data consolidation and unified vocabulary generator.
